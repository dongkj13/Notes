## 自然语言处理的基本任务

自然语言处理是关于计算机科学和语言学的交叉学科，常见的研究任务包括：
- 分词（Word Segmentation或Word Breaker，WB）
- 信息抽取（Information Extraction，IE）
- 关系抽取（Relation Extraction，RE）
- 命名实体识别（Named Entity Recognition，NER）
- 词性标注（Part Of Speech Tagging，POS）
- 指代消解（Coreference Resolution）
- 句法分析（Parsing）
- 词义消歧（Word Sense Disambiguation，WSD）
- 语音识别（Speech Recognition）
- 语音合成（Text To Speech，TTS）
- 机器翻译（Machine Translation，MT）
- 自动文摘（Automatic Summarization）
- 问答系统（Question Answering）
- 自然语言理解（Natural Language Understanding）
- 光学字符识别（Optical Character Recognition，OCR）
- 信息检索（Information Retrieval，IR）

早期的自然语言处理系统主要是基于人工撰写的规则，这种方法费时费力，且不能覆盖各种语言现象。上个世纪80年代后期，机器学习算法被引入到自然语言处理中，这要归功于不断提高的计算能力。研究主要集中在统计模型上，这种方法采用大规模的训练语料（corpus）对模型的参数进行自动的学习，和之前的基于规则的方法相比，这种方法更具鲁棒性。

## 统计语言模型

简单地说，语言模型就是给句子（词序列）分配一个概率。这个概率表示这句话“合理程度”。利用语言模型，可以确定哪个词序列出现的可能性更大，或者给定若干个词，预测下一个最可能出现的词语。

那么如何计算一个句子的概率呢？给定句子（词序列），

$$
S=W_1,W_2,...,W_n
$$

它的概率可以表示为：
$$
P(S)=P(W_1,W_2,...,W_n)=P(W_1)P(W_2|W_1)...P(W_n|W_1,W_2,...,W_{n-1})
$$
可是这样的方法存在两个致命的缺陷：

1. 參数空间过大：条件概率$P(W_n|W_1,W_2,...,W_{n-1})$的可能性太多，无法估算，不可能有用；
2. 数据稀疏严重：对于非常多词对的组合，在语料库中都没有出现，依据最大似然估计得到的概率将会是0。

因此需要近似的计算方法，常见的方法有n-gram模型方法、决策树方法、最大熵模型方法、最大熵马尔科夫模型方法、条件随机域方法、神经网络方法，等等。

### 马尔可夫假设

马尔可夫假设(Markov assumption)，即假设当前词出现的概率只依赖于前$n-1$个词，可以得到
$$
P(W_i|W_1,W_2,...,W_{i-1})=P(W_i|W_{i-n+1},...,W_{i-1})
$$
基于此思想，定义**n-gram**语言模型如下：

#### 一元模型 unigram

$$
P(W_1,W_2,...,W_n) = \prod_{i=1}^n P(W_i)
$$

#### 二元模型 bigram

$$
P(W_1,W_2,...,W_n) = \prod_{i=1}^n P(W_i|W_{i-1})
$$

#### 三元模型 trigram

$$
P(W_1,W_2,...,W_n) = \prod_{i=1}^n P(W_i|W_{i-2},W_{i-1})
$$

n越大，模型越准确，也越复杂，需要的计算量越大。最常用的是bigram，其次是unigram和trigram，n取≥4的情况较少。

### 模型参数估计

一般采用最大似然估计（Maximum Likelihood Estimation，MLE）的方法对模型的参数进行估计：
$$
P(W_i|W_{i-n+1},...,W_{i-1}) = \frac{C(W_{i-n+1},...,W_{i-1},W_i)}{C(W_{i-n+1},...,W_{i-1})}
$$
`C(X)`表示X在训练语料中出现的次数，训练语料的规模越大，参数估计的结果越可靠。

### 数据平滑

 数据平滑是对频率为0的n元对进行估计，典型的平滑算法有加法平滑、Good-Turing平滑、Katz平滑、插值平滑，等等。

### 优缺点

优点：

1. 采用极大似然估计，参数易训练；
2. 完全包含了前 n-1 个词的全部信息；
3. 可解释性强，直观易理解。

缺点：

1. 缺乏长期依赖，只能建模到前 n-1 个词；
2. 随着 n 的增大，参数空间呈指数增长；
3. 数据稀疏，难免会出现未登录词(OOV)的问题；
4. 单纯的基于统计频次，泛化能力差。

## 神经语言模型

Bengio在2003年发表的《[A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) 》论文中，提出了如下图所示的前馈神经网络结构

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gf2enyr56qj30ds0c13zj.jpg)

观察上图，假设有一组词序列：$W_1,W_2,...,W_n$，其中$W_i \in V$，$V$是所有单词的集合。我们的输入是一个词序列，而我们的输出是一个概率值，表示根据context预测出下一个词是$i$的概率。用数学来表示，我们最终是要训练一个模型：
$$
f(W_1,W_2,...,W_n) = P(W_t = i | context) = P(W_t | W_1^{t-1})
$$
模型结构分为三层，分别是输入层、隐含层、输出层。

输入层将输入词序列映射到词表C中，赋予它一个词向量表示。C实际上就是一个$|V|*m$的自由参数矩阵，$|V|$表示词表的大小，$m$表示每个词的维度。

隐含层是一个激活函数为tanh的全连接层。

输出层实际接收隐含层输出和输入层输出，输出结果采用softmax函数，计算如下：
$$
y = b + Wx + U \tanh(d+Hx)
$$

$$
P(W_t|W_{t-n+1},...,W_{t-1}) = \frac{e^{y_{w_t}}}{\sum_i e^{y_i}}
$$

### 优缺点

优点：

1. 长距离依赖，具有更强的约束性；
2. 避免了数据稀疏所带来的OOV问题；
3. 好的词表征能够提高模型泛化能力。

缺点：

1. 模型训练时间长；
2. 神经网络黑盒子，可解释性较差。

## 参考

- [语言模型的基本概念](https://www.cnblogs.com/Dream-Fish/p/3963028.html)
- [深入理解语言模型 Language Model](https://zhuanlan.zhihu.com/p/52061158)
- [语言模型：从n元模型到NNLM](https://zhuanlan.zhihu.com/p/43453548)
- [【论文阅读】A Neural Probabilistic Language Model](https://blog.csdn.net/u014568072/article/details/78557837)
- [【论文阅读】A Neural Probabilistic Language Model](https://www.cnblogs.com/Dream-Fish/p/3950024.html)

