## Attention Model（注意力模型）

- [深度学习中的注意力模型（2017版）](https://zhuanlan.zhihu.com/p/37601161)
- [Attention机制详解（一）——Seq2Seq中的Attention](https://zhuanlan.zhihu.com/p/47063917)
- [Attention机制详解（二）——Self-Attention与Transformer](https://zhuanlan.zhihu.com/p/47282410)
- [Attention机制详解（三）——Attention模型的应用](https://zhuanlan.zhihu.com/p/47613793)

## Transformer

[Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)

[翻译：Attention is all you need](https://www.yiyibooks.cn/yiyibooks/Attention_Is_All_You_Need/index.html)

PyTorch代码实现，比较容易看懂

- [Transformer模型的PyTorch实现](https://luozhouyang.github.io/transformer/)
- https://github.com/jadore801120/attention-is-all-you-need-pytorch

Tensorflow代码实现

- [《Attention is all you need》源码解析+算法详解](https://blog.csdn.net/Urbanears/article/details/98662838)

- https://github.com/Kyubyong/transformer
- https://github.com/princewen/tensorflow_practice/tree/master/basic/Basic-Transformer-Demo

图片解释非常清楚

- [Transform详解(超详细) Attention is All you need论文](https://zhuanlan.zhihu.com/p/63191028)
- [Attention Is All You Need | 源码解析（pytorch）](https://zhuanlan.zhihu.com/p/126671976)

位置编码

- [一文读懂Transformer模型的位置编码](https://zhuanlan.zhihu.com/p/106644634)
- [Transformer改进之相对位置编码(RPE)](https://zhuanlan.zhihu.com/p/105001610)

## 其他参考

- [论文解读:Attention is All you need](https://zhuanlan.zhihu.com/p/46990010)
- [一文看懂 Attention（本质原理+3大优点+5大类型）](https://zhuanlan.zhihu.com/p/91839581)
- [遍地开花的 Attention，你真的懂吗？](https://zhuanlan.zhihu.com/p/77307258)

