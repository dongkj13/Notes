[TOC]

# 1. 梯度下降法

## 1.1 梯度下降法原理

梯度下降法简单来说就是一种寻找目标函数最小化的方法。从数学的角度上看，梯度的方向是函数增长速度最快的方向，那么[**梯度的反方向就是函数减少最快的方向**](https://www.zybuluo.com/wzqh/note/620955)。即每次在目标函数的负梯度方向上，前进一定的步长，直到到达局部最优点（对于凸函数，就是全局最优点）。

因此引入两个概念：

1. 步长（Learning rate）：步长又称**学习率**，决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。
2. 损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为平方损失函数。

## 1.2 梯度下降法调优

在梯度下降法中调优比较重要的是3个因素，**步长、初始值、归一化**。

1. 步长：步长太小，收敛慢，步长太大，会远离最优解。所以需要从小到大，分别测试，选出一个最优解。
2. 初始值：随机选取初始值，当损失函数是非凸函数时，找到的解可能是局部最优解，需要多测试几次，从局部最优解中选出最优解。当损失函数是凸函数时，得到的解就是最优解。
3. 归一化：如果不归一化，会收敛的很慢，会形成之字的路线。

## 1.3 梯度下降法分类

### 1.3.1 批量梯度下降法（BGD）

计算梯度时**使用所有的样本**，这样每次算出来的梯度都是当前最优的方向。

- 优点

    - 迭代次数少

    - 若损失函数为凸函数，能够保证收敛到全局最优解；若为非凸函数，能够收敛到局部最优值
    - 易于并行实现

- 缺点
    - 训练速度慢（时间，每一次训练需要的时间）

    - 需要内存大（空间）
    - 不支持在线更新

### 1.3.2 随机梯度下降法（SGD）

随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有样本的数据，而是**仅仅选取一个样本**来求梯度。

![image](https://pic2.zhimg.com/80/v2-18df00ed2b351b531623715edadb1a6a_hd.jpg)

- 优点
    - 训练速度快

    - 支持在线更新
    - 有几率跳出局部最优解

- 缺点

    - 容易收敛到局部最优，并且容易被困在鞍点
    - 迭代次数多

### 1.3.3 小批量梯度下降法（MBGD）

小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，在每次更新参数时使用b个样本（b一般为10）。

# 2. 其他优化算法

## 2.1 牛顿法

**牛顿法又名切线法**，它的基本思想是对损失函数的**二阶泰勒展开**进行求导。

从本质上去看，牛顿法是**二阶收敛**，梯度下降是**一阶收敛**，所以牛顿法就更快。

如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了**局部的最优**，没有**全局思想**。）

切线法形象化如下：

![image](https://note.youdao.com/yws/api/personal/file/DAE974B02DE5484BB4F113CE2C3FB4AF?method=download&shareKey=a99e48ab1f08368621009e3dfc2d8218)

从几何上说，牛顿法就是用一个**二次曲面**去拟合你当前所处位置的局部曲面，而梯度下降法是用一个**平面**去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。

- 优点

    - 梯度下降的速度很快

- 缺点

    - 牛顿法是一种迭代算法，每一步都需要求解目标函数的**Hessian矩阵**的逆矩阵，计算比较复杂。

## 2.2 拟牛顿法

拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用**正定矩阵**来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时计算**目标函数的梯度（一阶导数）**。

- 优点

    - 使用正定矩阵模拟Hessian矩阵，使得迭代速度比BGD快

    - 只计算了函数的梯度，所以训练时间比牛顿法快

## 2.3 共轭梯度法

共轭梯度法是介于**最速下降法与牛顿法，拟牛顿法**之间的一个方法，它仅需利用**一阶导数信息**，但克服了**最速下降法收敛慢的缺点**，又避免了**牛顿法需要存储和计算Hesse矩阵并求逆的缺点**，克服了**拟牛顿法需要很大的存储空间**，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。

- 优点

    - 需存储量小
    - 稳定性高

    - 不需要任何外来参数

# 3. SGD延伸算法

## 3.1 Momentum（动量法）

Momentum旨在加速学习，特别是处理高曲率、小但一致的梯度，或带噪音的梯度。Momentum算法会观察历史梯度（动量），若当前梯度的方向与历史梯度一致（表明当前样本不太可能为异常点），则会增强这个方向的梯度，若当前梯度与历史梯方向不一致，则梯度会衰减。

![image](https://pic3.zhimg.com/80/v2-343e40136f6e80fea93f252981a560ec_hd.jpg)

以下给出一个形象的解释，我们把一个球推下山，球在下坡时积累惯性（动量），在途中若球运动方向不变，因为惯性，球会越来越快，若球的方向发生变化，因为惯性，球的速度会变慢。

左边是SGD，右边是Momentum旨在加速学习：

![image](https://note.youdao.com/yws/api/personal/file/F6FD7928D3ED4634B16B674991F52073?method=download&shareKey=1d5f19827fdd9de5377c35d66743fdbd)



## 3.2 Nesterov Momentum（NAG）

Nesterov Momentum是Momentum的变种，用于解决SGD容易陷入局部最优的问题。我们知道Momentum方法中梯度方向由积累的动量和当前梯度方法共同决定，与其看当前梯度方向，不妨先看看跟着积累的动量走一步是什么情况，再决定怎么走。

在小球向下滚动的过程中，我们希望小球能够提前知道在哪些地方坡面会上升，这样在**遇到上升坡面之前，小球提前就开始减速**，就不容易陷入局部最优解。

![image](https://note.youdao.com/yws/api/personal/file/FD3375BA6CB14956AACDB809DCAECC0D?method=download&shareKey=781f1954d9d615eada0ce30a10fe5cf9)

## 3.3 Adagrad

Adagrad是自适应梯度法。它通过记录每次迭代过程中的前进方向和距离，从而使得针对不同问题，有一套自适应调整学习率的方法，对于出现频率较低参数采用较大的α更新；相反，对于出现频率较高的参数采用较小的α更新。

![image](https://pic2.zhimg.com/80/v2-386e1f3eefd768c25ef6a79d547c98a1_hd.jpg)

## 3.4 RMSprop

Adagrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，因此可**缓解Adagrad算法学习率下降较快的问题**。

![img](https://pic4.zhimg.com/80/v2-b0294a1922789b3e99d1c8445c69dbba_hd.jpg)

## 3.5 Adam

Adam(Adaptive Moment Estimation)是另一种**自适应学习率**的方法。总结以上算法，以SGD作为最初的算法，Momentum在其基础上加入了一阶动量（历史梯度的累计），AdaGrad和RMSProp在其基础上加入了二阶动量（历史梯度的平方累计），Adam就是结合了一阶动量和二阶动量算法。

## 3.6 Nadam

我们说Adam是集大成者，而Nadam = Adam + NAG。

## 3.7 展示

介绍了上面的几种学习率的优化方法，大家可能一下不太好理解，这里放两张动图，帮助大家加深理解。

![image](https://note.youdao.com/yws/api/personal/file/A980FA97948346A68180FE015FEFE6F1?method=download&shareKey=bbd5b01ae989135ec56718e2f9d39c3a)

![image](https://note.youdao.com/yws/api/personal/file/4DC425463FB740F88F94F02852555B64?method=download&shareKey=7ed719a4c25de6e2128781c9e0302b55)

# 4. 参考

- [梯度下降法小结](https://www.cnblogs.com/huangyc/p/9801261.html)
- [梯度下降法的三种形式BGD、SGD以及MBGD](https://www.cnblogs.com/maybe2030/p/5089753.html)
- [深度学习中的优化算法](https://zhuanlan.zhihu.com/p/43506482)
