正则化是一种回归的形式，它将系数估计（coefficient estimate）朝零的方向进行约束、调整或缩小。也就是说，正则化可以在学习过程中降低模型复杂度和不稳定程度，从而避免过拟合的危险。

## LP范数

范数简单可以理解为用来表征向量空间中的距离，而距离的定义很抽象，只要满足非负、自反、三角不等式就可以称之为距离。

LP范数不是一个范数，而是一组范数，其定义如下：
$$
||x||_p=(\sum_i^n x_i^p)^{\frac{1}{p}}
$$

$p$的范围是$[1,∞)$。$p$在$(0,1)$范围内定义的并不是范数，因为违反了三角不等式。

根据$p$的变化，范数也有着不同的变化，借用一个经典的有关P范数的变化图如下： 

![img](https://images2018.cnblogs.com/blog/764050/201806/764050-20180630144218453-1100326347.jpg)

上图表示了$p$从0到正无穷变化时，单位球（unit ball）的变化情况。在P范数下定义的单位球都是凸集，但是当$0<p<1$时，在该定义下的unit ball并不是凸集。

那问题来了，L0范数是啥玩意？

L0范数表示向量中非零元素的个数，用公式表示如下：

$$
||x||_0=\#(i|x_i≠0)
$$

我们可以通过最小化L0范数，来寻找最少最优的稀疏特征项。但不幸的是，L0范数的最优化问题是一个NP hard问题（L0范数同样是非凸的）。因此，在实际应用中我们经常对L0进行凸松弛，理论上有证明，L1范数是L0范数的最优凸近似，因此通常使用L1范数来代替直接优化L0范数。

### L1范数

根据LP范数的定义我们可以很轻松的得到L1范数的数学形式：

$$
||x||_1=\sum_i^n |x_i|
$$

通过上式可以看到，L1范数就是向量各元素的绝对值之和，也被称为是"稀疏规则算子"（Lasso regularization）。那么问题来了，为什么我们希望稀疏化？稀疏化有很多好处，最直接的两个：

- 特征选择
- 可解释性

### L2范数

L2范数是最熟悉的，它就是欧几里得距离，公式如下：

$$
||x||_2=\sqrt{\sum_i^n(x_i)^2}
$$

L2范数有很多名称，有人把它的回归叫“岭回归”（Ridge Regression），也有人叫它“权值衰减”（Weight Decay）。以L2范数作为正则项可以得到稠密解，即每个特征对应的参数w都很小，接近于0但是不为0；此外，L2范数作为正则化项，可以防止模型为了迎合训练集而过于复杂造成过拟合的情况，从而提高模型的泛化能力。

## L1范数和L2范数的区别

### 解空间形状

引入PRML一个经典的图来说明下L1和L2范数的区别，如下图所示：

![img](https://images2018.cnblogs.com/blog/764050/201806/764050-20180630154806762-832587163.jpg)

 

如上图所示，蓝色的圆圈表示问题可能的解范围，橘色的表示正则项可能的解范围。而整个目标函数（原问题+正则项）有解当且仅当两个解范围相切。从上图可以很容易地看出，由于L2范数解范围是圆，所以相切的点有很大可能不在坐标轴上，而由于L1范数是菱形（顶点是凸出来的），其相切的点更可能在坐标轴上，而坐标轴上的点有一个特点，其只有一个坐标分量不为零，其他坐标分量为零，即是稀疏的。所以有如下结论，==L1范数可以导致稀疏解，L2范数导致稠密解。==

### 函数叠加

我们考虑一维的情况，横轴是参数的值，纵轴是损失函数，加入正则项之后，损失函数曲线图变化如下：



![img](https://upload-images.jianshu.io/upload_images/4155986-646760bb218f4ef4.png?imageMogr2/auto-orient/strip|imageView2/2/w/1041/format/webp)

可以看到，在加入L1正则项后，最小值在红点处，对应的w是0。而加入L2正则项后，最小值在黄点处，对应的w并不为0。

为什么呢？加入L1正则项后，目标函数变为$L(w)+C|w|$，单就正则项部分求导，原点左边的值为-C，原点右边的值为C，因此，只要原目标函数的导数绝对值$|L'(w)|<C$,那么带L1正则项的目标函数在原点左边部分始终递减，在原点右边部分始终递增，最小值点自然会出现在原点处。

加入L2正则项后，目标函数变为$L(w)+Cw^2$，只要原目标函数在原点处的导数不为0，那么带L2正则项的目标函数在原点处的导数就不为0，那么最小值就不会在原点。因此L2正则只有见效w绝对值的作用，但并不能产生稀疏解。

### 贝叶斯先验

从贝叶斯先验的角度看，当训练一个模型时，仅依靠当前的训练数据集是不够的，为了实现更好的泛化能力，往往需要加入先验项，而加入正则项相当于加入了一种先验。具体数学推导可参考《[L1和L2正则化的概率解释](https://zhuanlan.zhihu.com/p/56185913)》。

- L1范数相当于加入了一个Laplacean先验；
- L2范数相当于加入了一个Gaussian先验。

如下图所示：

![img](https://images2018.cnblogs.com/blog/764050/201806/764050-20180630155315995-527002822.png)

可以看到，当均值为0时，高斯分布在极值点处是平滑的，也就是高斯先验分布认为w在极值点附近取不同值的可能性是接近的。但对拉普拉斯分布来说，其极值点处是一个尖峰，所以拉普拉斯先验分布中参数w取值为0的可能性要更高。

## 为什么L2正则可以提升模型的泛化能力？

到目前为止，我们只是解释了L2正则化项有让w“变小”的效果，但是还没解释为什么w“变小”可以防止overfitting？一个所谓“显而易见”的解释就是：更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合刚刚好（这个法则也叫做奥卡姆剃刀），而在实际应用中，也验证了这一点，L2正则化的效果往往好于未经正则化的效果。当然，对于很多人（包括我）来说，这个解释似乎不那么显而易见，所以这里添加一个稍微数学一点的解释（引自知乎）：

过拟合的时候，拟合函数的系数往往非常大，为什么？如下图所示，过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。

![img](https://upload-images.jianshu.io/upload_images/2027163-acf080fbcf329132.png?imageMogr2/auto-orient/strip|imageView2/2/w/266/format/webp)

而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。

## 参考

- [正则化](https://www.cnblogs.com/maybe2030/p/9231231.html)
- [带答案面经分享-L1正则&L2正则](https://www.jianshu.com/p/7d5997878520)