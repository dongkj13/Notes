[TOC]

## 朴素贝叶斯的理论基础

朴素贝叶斯算法是基于**贝叶斯定理**与**特征条件独立假设**的分类方法。

### 贝叶斯定理
$P(A|B)$表示事件B已经发生的前提下，事件A发生的概率，叫做事件B发生下事件A的**条件概率**。其基本求解公式为：

$$
P(A|B)=\frac{P(AB)}{P(B)}
$$

**贝叶斯定理**便是基于条件概率，通过$P(A|B)$来求$P(B|A)$：

$$
P(B|A)=\frac{P(A|B)P(B)}{P(A)}
$$

顺便提一下，上式中的分母$P(A)$，可以根据**全概率公式**分解为：

$$
P(A)=\sum_{i=1}^n P(B_i)P(A|B_i)
$$

### 特征条件独立假设
给定训练数据集$(X,Y)$，其中每个样本$x$都包括n维特征，即$x=(x_1,x_2,x_3,...,x_n)$，类标记集合含有k种类别，即$y=(y_1,y_2,...,y_k)$。

如果现在来了一个新样本$x$，我们要怎么判断它的类别？从概率的角度来看，这个问题就是给定$x$，它属于哪个类别的概率最大。那么问题就转化为求解$P(y_1|x),P(y_2|x),...,P(y_k|x)$中最大的那个，即求后验概率最大的输出：

$$
\arg max_{y_k} P(y_k|x)
$$

那么根据贝叶斯定理，有：

$$
P(y_k|x)=\frac{P(x|y_k)P(y_k)}{P(x)}=\frac{P(x|y_k)P(y_k)}{\sum_k P(x|y_k)P(y_k)}
$$

分子中的$P(y_k)$是先验概率，根据训练集就可以简单地计算出来。

而条件概率$P(x|y_k)=P(x_1,x_2,...,x_n|y_k)$，它的参数规模是指数数量级别的，假设第$i$维特征$x_i$可取值的个数有$S_i$个，类别取值个数为$k$个，那么参数个数为：$k\prod^n_{i=1} S_i$。

这显然不可行。针对这个问题，朴素贝叶斯算法对条件概率分布作出了**独立性的假设**，通俗地讲就是说假设各个维度的特征$x_1,x_2,...,x_n$互相独立，在这个假设的前提上，条件概率可以转化为：

$$
P(x|y_k)=P(x_1,x_2,...,x_n|y_k)=\prod^n_{i=1}P(x_i|y_k)
$$

这样，参数规模就降到$k \sum^n_{i=1} S_i$。

于是**朴素贝叶斯分类器**可表示为：

$$
f(x)=\arg max_{y_k}P(y_k|x)=\arg max_{y_k}\frac{P(y_k) \prod^n_{i=1}P(x_i|y_k)}{P(x)}=\arg max_{y_k} P(y_k) \prod^n_{i=1}P(x_i|y_k)
$$

因为对所有的$y_k$，上式中的分母的值都是一样的（即样本$x$出现的概率，与类别无关），所以可以忽略分母部分

关于$P(y_k)$，$P(x_i|y_k)$的求解，有以下三种常见的模型。

## 三种常见的模型

### 多项式模型

- 当特征是离散的时候，使用多项式模型。
- 多项式模型在计算先验概率$P(y_k)$和条件概率$P(x_i|y_k)$时，会做一些平滑处理，具体公式为：

$$
P(y_k)= \frac{N_{y_k} + \alpha}{N + k\alpha} \\
P(x_i|y_k) = \frac{N_{y_k, x_i} + \alpha}{N_{y_k} + n \alpha}
$$

### 高斯模型
- 当特征是连续变量的时候，使用高斯模型
- 高斯模型假设每一维特征都服从高斯分布（正态分布）：
$$
P(x_{i}|y_{k})=\frac{1}{\sqrt{2\pi\sigma_{y_{k},i}^{2}}}e^{-\frac{(x_{i}-\mu_{y_{k},i})^{2}}{2 \sigma_{y_{k},i}^{2}}}
$$
- $\mu_{y_{k},i}$表示类别为$y_{k}$的样本中，第i维特征的均值。 $\sigma_{y_{k},i}^{2}$表示类别为$y_{k}$的样本中，第i维特征的方差。

### 伯努利模型
- 与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是1和0（以文本分类为例，某个单词在文档中出现过，则其特征值为1，否则为0）.
- 伯努利模型中，条件概率$P(x_{i}|y_{k})$的计算方式是：
    - 当特征值$x_{i}$为1时，$P(x_{i}|y_{k})=P(x_{i}=1|y_{k})$；
    - 当特征值$x_{i}$为0时，$P(x_{i}|y_{k})=1-P(x_{i}=1|y_{k})$；

## 与LR区别
- 朴素贝叶斯是**生成模型**，根据已有样本进行贝叶斯估计学习出先验概率$P(Y)$和条件概率$P(X|Y)$，进而求出联合分布概率$P(XY)$，最后利用贝叶斯定理求解$P(Y|X) $
    - 朴素贝叶斯是基于很强的条件独立假设（在已知分类Y的条件下，各个特征变量取值是相互独立的）
- LR是**判别模型**，根据极大化对数似然函数直接求出条件概率$P(Y|X)$

## 优缺点
- 优点
    - 朴素贝叶斯适用于数据集少的情景，适合多分类任务，适合增量式训练
    - 所需估计参数很少，对缺失数据不太敏感
    - 朴素贝叶斯的条件概率计算彼此是独立的，因此特别适于分布式计算
    - 收敛速度将快于判别模型
- 缺点
    - 特征之间相互独立这个假设在实际应用中往往是不成立的，因此分类的性能不一定很高
    - 对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）

## 参考资料
- [朴素贝叶斯理论推导与三种常见模型](http://blog.csdn.net/u012162613/article/details/48323777)