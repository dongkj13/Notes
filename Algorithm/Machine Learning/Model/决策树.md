[TOC]

决策树的学习通常包括：特征选择、决策树的生成、决策树的剪枝。

# 三类决策树算法

## ID3
熵是表示随机变量的不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为：

$$
P(X=x_i)=p_i, \quad i=1,2,…,n
$$

**随机变量X的熵**定义为：

$$
H(X)=−\sum_{i=1}^n p_i \log p_i
$$

上式中，若pi=0，定义0log0=0。对数底可以为2或者e，这时熵的单位分别为比特或纳特。由定义，熵只依赖于 X 的分布，与 X 的取值无关，所以也可以记为：

$$
H(p)=−\sum_{i=1}^n p_i \log p_i
$$

熵越大，随机变量的不确定性越大，所以：$ 0≤H(p)≤\log n $。

对于随机变量 (X,Y)，其联合概率分布：

$$
P(X=x_i,Y=y_j)=P_{ij},\quad i=1,2,…,n;j=1,2,…,m
$$

**条件熵**表示在已知随机变量X的条件下，变量Y的不确定性，定义为X给定条件下Y的条件概率分布的熵对X的数学期望：

$$
H(Y|X)=\sum_{i=1}^n p_i H(Y|X=X_i)
$$

这里$p_i=P(X=x_i),\quad i=1,2,…,n$。

**信息增益**表示得知特征X的信息而是类Y的信息的不确定性减少的程度，也称为互信息。

特征A对训练数据集D的信息增益g(D,A)定义为集合D的经验熵H(D)与特征A给定条件下D的检验条件熵之差，即：

$$
g(D,A)=H(D)−H(D|A)
$$

因此，决策树特征选择的方法可以为：对训练集D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

## C4.5
但是，以信息增益作为划分的特征，可能存在偏向于选择取值较多的特征的问题，因此可以改为使用**信息增益比**：

$$
g_R(D,A)=\frac{g(D,A)}{H_A(D)}
$$

其中，$H_A(D)=-\sum_{i=1}^n \frac {|D_i|}{|D|}\log_2 \frac {|D_i|}{|D|}$

## CART

### CART分类树
CART分类树用的是另外一个指标 – 基尼指数.
假设一共有K个类,样本属于第k类的概率是$p_k$,则概率分布的**基尼指数**定义为:
$$
Gini(p)=\sum_{k=1}^K p_k(1−p_k)=1−\sum_{k=1}^K p^2_k
$$

对于二类分类问题,若样本属于正类的概率为 p,则基尼指数为:

$$
Gini(p)=2p(1−p)
$$

对于给定的样本集合D,其基尼指数定义为:

$$
Gini(D)=1−\sum_{k=1}^K (\frac{|C_k|}{|D|})^2
$$

其中$C_k$是D中属于第k类的样本子集.

如果样本集合D被某个特征A是否取某个值分成两个样本集合D1和D2,即

$$
D_1 =\{(x,y) \in D | A(x) = a\}, \quad D_2 = D - D_1
$$

则在特征A的条件下,集合D的基尼指数定义为:

$$
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
$$

基尼指数Gini(D)反应的是集合D的不确定程度，跟熵的含义相似。Gini(D,A)反应的是经过特征A划分后集合D的不确定程度。所以决策树分裂选取Feature的时候，要选择使基尼指数最小的Feature。

### CART回归树
假设X和Y分别为输入和输出变量，Y为连续变量，训练数据集D为：

$$
D=\{(X_1,y_1),(X_2,y_2),…,(X_n,y_n)\}
$$

一个回归树对应着输入空间的一个划分以及在划分的单元上的输出值。加入已经将输入空间划分为M个单元$R_1,R_2,…,R_M$，在每个单元$R_m$上有个固定的输出$c_m$，则回归树表示为：

$$
f(X)=\sum_{m=1}^M c_mI(X \in R_m)
$$

问题是怎么对输入空间进行划分.一般采用启发式的思路，选择第j个特征Xj和他的取值s分别作为切分变量和切分点，并定义两个区域：

$$
R_1(j,s)={X|X^{(j)}≤s},\quad R_2(j,s)={X|X^{(j)}>s}
$$

然后采用**最小平方误差准则**求解最优的切分变量j和切分点s。

$$
\min \limits_{j,s}[\min \limits_{c_1} \sum _{X_i\in R_1(j,s)} (y_i−c_1)^2 + \min \limits_{c_2} \sum_{X_i \in R_2(j,s)} (y_i−c_2)^2]
$$

别看上面的式子很复杂，实际上不难理解，每一个切分变量和切分点对(j,s)都将输入空间分成两个区域，然后分别求每个区域的输出值，使得误差最小，很显然输出值应该是那个区域所有样本值的平均值，即：

$$
c_1=ave(y_i|X_i \in R_1(j,s)), \quad c_2=ave(y_i| X_i \in R_2(j,s))
$$

然后每个(j,s)对里找出使总误差最小的对作为最终的切分变量和切分点,对切分后的子区域重复这一步骤。

## 比较

|     区别     |   ID3    |      C4.5      |      CART      |
| :----------: | :------: | :------------: | :------------: |
| 纯度量化指标 | 信息增益 |   信息增益率   |    基尼系数    |
|     数据     | 离散数据 | 连续数据离散化 | 连续数据离散化 |
|   树的类型   |  多叉树  |     多叉树     |     二叉树     |


# 特殊值处理

## 连续值处理
相比ID3，C4.5还能处理连续属性值，具体步骤为:
1. 把需要处理的样本（对应根节点）或样本子集（对应子树）按照连续变量的大小从小到大进行排序.
2. 假设该属性对应的不同的属性值一共有N个,那么总共有N−1个可能的候选分割阈值点,每个候选的分割阈值点的值为上述排序后的属性值中两两前后连续元素的中点,根据这个分割点把原来连续的属性分成bool属性.实际上可以不用检查所有N−1个分割点,具体请看下面的例子.
3. 用信息增益比率选择最佳划分.

连续属性值比较多的时候，由于需要排序和扫描，会使C4.5的性能有所下降。

## 缺失值处理

$$
g(D,A)=\rho \times g(\tilde{D},A)
$$

其中$\rho$表示无缺失值样本所占的比例，$g(\tilde{D},A)$表示在无缺失值样本子集上的信息增益。



# 剪枝
todo



# 过拟合解决办法
- 剪枝
- 随机森林
    - 构建大量的决策树组成森林来防止过拟合；虽然单个树可能存在过拟合，但通过广度的增加就会消除过拟合现象


# 优缺点
- 优点：
    - 计算简单，易理解，可解释性强，树的结构可以可视化出来
    - 适合处理缺失属性的样本，对样本的类别要求不高（可以是数值，布尔，文本等混合样本）
    - 能处理不相干特征
- 缺点：
    - 容易过拟合
    - 忽略了数据之间的相关性
    - 不支持在线学习
    - 决策树可能是不稳定的，因为数据中的微小变化可能会导致完全不同的树生成。这个问题可以通过决策树的集成来得到缓解


# 参考
- [数据挖掘面试题之决策树必知必会](https://www.jianshu.com/p/fb97b21aeb1d)