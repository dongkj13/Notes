[TOC]

# 集成学习
集成学习(ensemble learning)本身不是一个单独的机器学习算法，而是通过构建并结合多个机器学习器来完成学习任务。集成学习可以用于分类问题集成，回归问题集成，特征选取集成，异常点检测集成等等。

集成学习有两个主要的问题需要解决，第一是如何得到若干个个体学习器，第二是如何选择一种结合策略，将这些个体学习器集合成一个强学习器，也就是模型融合策略。

按照个体学习器的关系可以分为两类：
- 个体学习器间不存在强依赖关系、可同时生成的并行化方法、用于**减少方差**的bagging
- 个体学习器问存在强依赖关系、必须串行生成的序列化方法、用于**减少偏差**的boosting

## Boosting

### Adaboost

#### 正则化
- 加入正则化项，即步长(learning rate)

#### 优缺点
- 优点
    - Adaboost作为分类器时，分类精度很高
    - 在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。
    - 能够基于泛化性能相当弱的的学习器构建出很强的集成，**不容易发生过拟合**。
    - 作为简单的二元分类器时，构造简单，结果可理解。
- 缺点
    - **对异常样本比较敏感**，异常样本在迭代过程中会获得较高的权值，影响最终学习器的性能表现。

#### 面试题
> AdaBoost几种基本机器学习算法哪个抗噪能力最强，哪个对重采样不敏感？


> 训练过程中，每轮训练一直存在分类错误的问题，整个Adaboost却能快速收敛，为何？

每轮训练结束后，AdaBoost会对样本的权重进行调整，调整的结果是越到后面被错误分类的样本权重会越高。而后面的分类器为了达到较低的带权分类误差，会把样本权重高的样本分类正确。这样造成的结果是，虽然每个弱分类器可能都有分错的样本，然而整个 AdaBoost 却能保证对每个样本进行正确分类，从而实现快速收敛。


### GBDT

不同的损失函数和极小化损失函数方法决定了 boosting 的最终效果
![image](https://tva1.sinaimg.cn/large/007S8ZIlly1gf2tdnowenj30hl031ac2.jpg)

Adaboost强调Adaptive（自适应），通过不断修改样本权重（增大分错样本权重，降低分对样本权重），不断加入弱分类器进行boosting。

而GBDT则是旨在不断减少残差（回归），通过不断加入新的树旨在在残差减少（负梯度）的方向上建立一个新的模型。——即损失函数是旨在最快速度降低残差。

#### 正则化
- 加入正则化项，即步长(learning rate)
- 通过不放回子采样，推荐在[0.5, 0.8]之间
- 对于弱学习器即CART回归树进行正则化剪枝

#### 优缺点
- 优点
    - **可以很好的处理缺失特征**。而逻辑回归和 SVM 没这样的天然特性。
    - 可以灵活处理各种类型的数据，包括连续值和离散值。同样逻辑回归和 SVM 没这样的天然特性。
    - **对异常值的鲁棒性非常强**。同样逻辑回归和 SVM 没有这样的天然特性。
    - 数据规模影响不大，因为我们对弱分类器的要求不高，作为弱分类器的决策树的深 度一般设的比较小，即使是大数据量，也可以方便处理。像 SVM 这种数据规模大的时候训练会比较麻烦
- 缺点
    - 由于弱学习器之间存在依赖关系，**难以并行训练数据**。不过可以通过自采样的SGBT来达到部分并行。

### XGBOOST
陈天奇的论文《[XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754v1.pdf)》以及作者讲座PPT《[Introduction to Boosted Trees ](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)》

#### 优缺点

- 优点
  - **精度更高：**GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；
  - **灵活性更强：**GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，（使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题））。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导；
  - **正则化：**XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；
  - **Shrinkage（缩减）：**相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间
  - **列抽样：**XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算
  - **缺失值处理：**XGBoost 采用的稀疏感知算法极大的加快了节点分裂的速度
  - **可以并行化操作：**块结构可以很好的支持并行计算。
- 缺点
  - 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；
  - 预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。
### Lightgbm
TODO

## Bagging
- 从原始样本集中使用Bootstraping方法随机抽取n个训练样本，共进行t轮抽取，得到t个训练集。（k个训练集之间相互独立，元素可以有重复）
- 对于t个训练集，我们训练t个模型（这t个模型可以根据具体问题而定，比如决策树，knn等）
    - 一般不用LR，SVM等强学习器，本身方差不大，甚至因为采样问题更难收敛
- 对于分类问题：由投票表决产生分类结果；对于回归问题：由k个模型预测结果的均值作为最后预测结果。（所有模型的重要性相同）


### 随机森林 Random Forest

> 随机森林是bagging的一个特化进阶版，所谓的特化是因为随机森林的弱学习器都是决策树。所谓的进阶是随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择，其基本思想没有脱离bagging的范畴。

#### 优缺点
- 优点
    - **训练可以高度并行化**，对于大数据时代的大样本训练速度有优势。
    - 两个随机性的引入，使得随机森林不容易陷入过拟合，具有很好的抗噪声能力
    - 能够处理很高维度特征的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化
    - 在训练后，可以给出各个特征对于输出的重要性
    - 由于采用了随机采样，训练出的模型的**方差小，泛化能力强**。
    - 在创建随机森林的时候，对generlization error使用的是无偏估计，模型泛化能力强
    - 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。
    - **处理高维数据，处理特征遗失数据，处理不平衡数据**
    - 在训练过程中，能够检测到feature间的互相影响
- 缺点
    - 随机森林在解决回归问题时，并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续的输出。
    - 在某些噪音比较大的样本集上，RF模型容易陷入过拟合。
    - 取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。
    - 可能有很多相似的决策树，掩盖了真实的结果。


# 模型融合

## 平均法

对于数值类的回归预测问题，通常使用的结合策略是平均法，也就是说，对于若干个弱学习器的输出进行平均得到最终的预测输出。常见的如**算术平均**，**加权平均**。

## 投票法

对于分类问题的预测，我们通常使用的是投票法。

最简单的投票法是**相对多数投票法**，也就是我们常说的少数服从多数，也就是T个弱学习器的对样本x的预测结果中，数量最多的类别ci为最终的分类类别。如果不止一个类别获得最高票，则随机选择一个做最终类别。

稍微复杂的投票法是**绝对多数投票法**，也就是我们常说的要票过半数。在相对多数投票法的基础上，不光要求获得最高票，还要求票过半数。否则会拒绝预测。

更加复杂的是**加权投票法**，和加权平均法一样，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别为最终类别。

## 学习法

### Stacking

当训练集数据很多时，可以使用更为强大的stacking融合方法，它的将多个初级学习器（基模型）的输出作为新的训练集，用来训练一个新的二级模型。一般情况下，初级学习器是不同类型的算法。直接使用初级学习器对训练集的打分作为变量进行训练时，过拟合风险很大，因此一般使用交叉验证法产生新的训练集。

![image](https://img-blog.csdnimg.cn/2019032215243260.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI5Njk0MTI=,size_16,color_FFFFFF,t_70)

详细文字说明请参考:
- [模型融合：bagging、Boosting、Blending、Stacking](https://blog.csdn.net/u012969412/article/details/76636336)
- [【机器学习】集成学习之stacking](https://blog.csdn.net/qq_32742009/article/details/81366768)

### Blending

Blending与Stacking大致相同，只是Blending的主要区别在于训练集不是通过K-Fold的CV策略来获得预测值从而生成第二阶段模型的特征，而是建立一个Holdout集，例如说10%的训练数据，第二阶段的stacker模型就基于第一阶段模型对这10%训练数据的预测值进行拟合。说白了，就是把Stacking流程中的K-Fold CV 改成 HoldOut CV。

# 参考

- [集成学习原理小结](https://www.cnblogs.com/pinard/p/6131423.html)
- [Adaboost算法原理小结](https://www.cnblogs.com/pinard/p/6133937.html)
- [梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html)
- [GBDT算法-回归原理以及实例理解](https://blog.csdn.net/zpalyq110/article/details/79527653)
- [GBDT算法-分类原理](https://nbviewer.jupyter.org/github/liudragonfly/GBDT/blob/master/GBDT.ipynb)
- [GBDT算法-分类原理以及实例理解](https://blog.csdn.net/qq_22238533/article/details/79192579)
- [GBDT的面试考核点](https://www.cnblogs.com/bnuvincent/p/9693190.html)
- [【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）](https://zhuanlan.zhihu.com/p/87885678)
- [xgboost原理分析以及实践(含手动计算还原xgboost过程)](https://blog.csdn.net/qq_22238533/article/details/794775476)
- [XGBoost超详细推导，终于有人讲明白了！(汇总公式图)](https://cloud.tencent.com/developer/article/1513111)
- [左手论文 右手代码 深入理解网红算法XGBoost](https://zhuanlan.zhihu.com/p/91817667)
- [Bagging与随机森林算法原理小结](https://www.cnblogs.com/pinard/p/6156009.html)
- [随机森林算法](http://www.zilhua.com/629.html)
- [随机森林](https://www.cnblogs.com/34fj/p/9018329.html)