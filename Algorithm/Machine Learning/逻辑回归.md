[TOC]

# 1. 指数族分布

指数族分布 (The exponential family distribution)，区别于指数分布（exponential distribution)。在概率统计中，若某概率分布满足下式，我们就称之属于指数族分布。 

$$
p(y;η)=b(y)\exp(η^TT(y)−a(η))
$$

其中$η$是natural parameter， $T(y)$是充分统计量，$\exp(−a(η))$是起到归一化作用。 确定了$T,a,b$我们就可以确定某个参数为$η$的指数族分布. 

统计中很多熟悉的概率分布都是指数族分布的特定形式，如伯努利分布，高斯分布，多项分布（multionmal)，泊松分布等。

## 1.1 伯努利分布 

$$
\begin{align*}
p(y;ϕ) &= ϕ^y(1−ϕ)^{1−y}\\
&=\exp[y \log ϕ+(1−y)\log (1−ϕ)]\\
&=\exp[y \log \frac{ϕ}{1−ϕ}+\log (1−ϕ)]\\
\end{align*}
$$

把伯努利分布可以写成指数族分布的形式，且 

$$
T(y)=y\\
η=\log \frac{ϕ}{1−ϕ}\\
a(η)=−\log⁡(1−ϕ)=\log⁡(1+e^η)\\
b(y)=1
$$

同时我们可以看到$ϕ=\frac{1}{1+e^{−η}}$，居然是logistic sigmoid的形式，后面在讨论LR是广义线性模型时，也会用到。

## 1.2 高斯分布

高斯分布也可以写为指数族分布的形式如下：

$$
\begin{align*}
p(y;μ)&=\frac{1}{\sqrt{2π}} \exp(−\frac{1}{2} (y−μ)^2)\\
&=\frac{1}{\sqrt{2π}} \exp (−\frac{1}{2} y^2) \exp (μy − \frac{1}{2} μ^2)
\end{align*}
$$

我们假设方差为1，当然不为1的时候也是可以推导的。上述我们就把高斯分布写为了指数族分布的形式，对应的 

$$
T(y)=y\\
η=μ\\
a(η)=μ^2/2=η^2/2\\
b(y)=\frac{1}{\sqrt{2π}} \exp(−\frac{1}{2} y^2)
$$

# 2. 广义线性模型 (Generalized linear model, GLM)
本节将讲述广义线性模型的概念，以及LR,最小二乘为何也属于广义线性模型。

考虑一个分类或回归问题，我们就是想预测某个随机变量$y$，$y$ 是某些特征(feature)$x$的函数。为了推导广义线性模式，我们必须做出如下三个假设：

1. $p(y|x;θ)$服从指数族分布
2. 给了$x$，我们的目的是为了预测$T(y)$的在条件$x$下的期望。一般情况$T(y)=y$，这就意味着我们希望预测$h_θ(x)=E[T(y)|x]=E[y|x]$
3. 参数$η$和输入$x$是线性相关的：$η=θ^T x$

在这三个假设（也可以理解为一种设计）的前提下，我们可以推导出一系列学习算法，称之为广义线性模型(GLM)。下面我们可以推导出一系列算法，称之为广义线性模型GLM. 下面举两个例子：

## 2.1 线性回归
假设$p(y|x;θ)∼N(μ,σ2)$，$μ$可能依赖于$x$，那么 

$$
\begin{align*}
h_θ(x)&=E[y|x;θ]\\  
&=μ\\
&=η\\
&=θ^Tx
\end{align*}
$$

第一行因为假设2，第二行因为高斯分布的特点，第三行根据上面高斯分布为指数族分布的推导，第四行因为假设3。

## 2.2 逻辑回归

考虑LR二分类问题，$y∈0,1$，因为是二分类问题，我们很自然的选择$p(y|x;θ) \sim Bernoulli(ϕ)$，即服从伯努利分布。那么 

$$
\begin{align*}
h_θ(x)&=E[y|x;θ]\\
&=ϕ\\
&=\frac{1}{1+e^{−η}}\\
&=\frac{1}{1+e^{−θ^Tx}}
\end{align*}
$$

第一行因为假设2，第二行因为伯努利分布的性质，第三行因为伯努利分布为指数族分布时的推导，第四行因为假设3。

因此逻辑回归需要满足以下两个假设：

1. 在假设预测值$y$服从伯努利分布，即正样本概率为$ϕ$，负样本概率为$1-ϕ$
2. 利用广义线性模型的假设，预测函数$h_θ(x)=E[y|x;θ]$，符合逻辑回归模型

# 3. 逻辑回归损失函数

实际上损失函数$J(θ)$是通过极大似然估计推导得到的。

由于$y$只能取0或1，服从二项分布，则后验概率 
$$
p(y|x;θ)=(h_θ(x))^y (1−h_θ(x))^{1−y}
$$

对于m个独立同分布的训练样本$x$，其似然函数写作

$$
L(θ)=\prod_{i=1}^m p(y_i|x_i;θ)= \prod_{i=1}^m h_θ(x_i)^{y_i}(1−h_θ(x_i))^{1−y_i}
$$

为了方便操作，取对数，则对数似然函数为

$$
l(θ)=\log ⁡L(θ)=\sum_{i=1}^m y_i \log ⁡h_θ(x_i) + (1−y_i) \log⁡(1−h_θ(x_i))
$$

根据“最大似然估计”，求$l(θ)$取最大值时的$θ$，定义损失函数$J(θ)$为

$$
J(θ)=−\frac{1}{m}l(θ)
$$

采用随机梯度下降，损失函数对于$θ_j$求偏导：
$$
\begin{align*}
\frac{\partial J(\theta)}{\partial \theta_j} &= −\frac{1}{m} \frac{\sum_{i=1}^m y_i \log ⁡h_θ(x_i) + (1−y_i) \log⁡(1−h_θ(x_i))}{\partial \theta_j}\\
&= −\frac{1}{m} \sum_{i=1}^m y_i \frac{1}{⁡h_θ(x_i)} \frac{\partial ⁡h_θ(x_i)}{\partial \theta_j} + (1−y_i) \frac{1}{⁡1 - h_θ(x_i)} \frac{\partial ⁡(1−h_θ(x_i))}{\partial \theta_j}\\
&= −\frac{1}{m} \sum_{i=1}^m (\frac{y_i}{⁡h_θ(x_i)} - \frac{1−y_i}{⁡1 - h_θ(x_i)}) \frac{\partial ⁡h_θ(x_i)}{\partial \theta_j}\\
&= −\frac{1}{m} \sum_{i=1}^m (\frac{y_i}{⁡h_θ(x_i)} - \frac{1−y_i}{⁡1 - h_θ(x_i)}) ⁡h_θ(x_i) (⁡1 - h_θ(x_i)) x_i^{(j)}\\
&= −\frac{1}{m} \sum_{i=1}^m (y_i - h_θ(x_i)) x_i^{(j)}\\
&= \frac{1}{m} \sum_{i=1}^m (h_θ(x_i) - y_i) x_i^{(j)}
\end{align*}
$$

$θ_j$的迭代公式：
$$
\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j} = \theta_j - \alpha \sum_{i=1}^m (h_θ(x_i) - y_i) x_i^{(j)}
$$



# 4. 对逻辑回归的进一步提问

- 逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？
    - 损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。至于原因大家可以求出这个式子的梯度更新
    $$
    \theta_j=\theta_j−(h_\theta(x_i;\theta) - y_i)∗x_i^j
    $$
    - 这个式子的更新速度只和$x_i^j$，$y_i$相关。和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。
    - 为什么不选平方损失函数的呢？其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmod函数本身的梯度是很相关的。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。

- 逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？
    - 先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。
    - 但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。
    - 如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。

- 为什么我们还是会在训练的过程当中将高度相关的特征去掉？
    - 去掉高度相关的特征会让模型的可解释性更好
    - 可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。

# 5. 优缺点

- 优点：
    - 形式简单，模型的**可解释性非常好**。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
    - 模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。
    - 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
    - 资源占用小，尤其是内存。因为只需要存储各个维度的特征值。
    - 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。
- 缺点:
    - 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
    - **很难处理数据不平衡的问题**。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1。我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。
    - **处理非线性数据较麻烦**。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。
    - 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。

# 6. 参考

- [机器学习算法之：指数族分布与广义线性模型](https://blog.csdn.net/u011467621/article/details/48197943)
- [逻辑回归的常见面试点总结](https://www.cnblogs.com/ModifyRong/p/7739955.html)
- [机器学习之Logistic回归(逻辑蒂斯回归)](https://blog.csdn.net/sinat_35512245/article/details/54881672)
- [逻辑回归（Logistic Regression, LR）简介](https://blog.csdn.net/jk123vip/article/details/80591619)
- [机器学习--LR逻辑回归与损失函数理解](https://blog.csdn.net/u014106644/article/details/83660226)

