## 定义
模型在训练集上效果好；在测试集上效果差，即模型泛化能力弱。


## 产生原因
1. 训练集的数量级和模型的复杂度不匹配，即训练集的数量级低，模型的复杂度高。
2. 训练集中的噪声数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系。
3. 训练集和测试集特征分布不一致。
4. 权值学习迭代次数足够多(Overtraining)，拟合了训练数据中的噪声和训练样例中没有代表性的特征。

## 解决方案
### 1. simpler model structure 简化模型
调小模型复杂度，使其适合自己训练集的数量级。例如在多项式模型中降低次数，在决策树模型中控制树的深度以及剪枝。

### 2. data augmentation 数据增广
训练集越多，过拟合的概率越小。在计算机视觉领域中，增广的方式是对图像旋转，缩放，剪切，添加噪声等。

### 3. regularization 正则化
模型参数太多，会导致我们的模型复杂度上升，容易过拟合，也就是我们的训练误差会很小。正则化是指通过引入额外新信息来解决机器学习中过拟合问题的一种方法。这种额外信息通常的形式是模型复杂性带来的惩罚度。正则化可以保持模型简单，另外，规则项的使用还可以约束我们的模型的特性。

常见的正则化方式，如L1正则可以使权重稀疏，L2正则可以使权重平滑。

### 4. dropout 随机失活

这个方法在神经网络里面很常用。dropout方法是ImageNet中提出的一种方法，就是让神经网络在前向传播的时候，让某个神经元的激活值以一定的概率P，让他停止工作，也就是将这个神经元的激活值变为0。Dropout是非常有效的减少过拟合的方法，通俗的讲当我们挡住了数据的一部分，模型仍然能判断出数据是什么的话，说明模型的能力已经很强。

### 5. early stopping 提前终止
对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如梯度下降（Gradient descent）学习算法。Early stopping便是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。

Early stopping方法的具体做法是，在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练。这种做法很符合直观感受，因为accurary都不再提高了，在继续训练也是无益的，只会提高训练的时间。那么该做法的一个重点便是怎样才认为validation accurary不再提高了呢？并不是说validation accuracy一降下来便认为不再提高了，因为可能经过这个Epoch后，accuracy降低了，但是随后的Epoch又让accuracy又上去了，所以不能根据一两次的连续降低就判断不再提高。一般的做法是，在训练的过程中，记录到目前为止最好的validation accuracy，当连续10次Epoch（或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。此时便可以停止迭代了（Early Stopping）。这种策略也称为“No-improvement-in-n”，n即Epoch的次数，可以根据实际情况取，如10、20、30……

### 6. ensemble 集成学习
集成学习算法也可以有效的减轻过拟合。Bagging通过平均多个模型的结果，来降低模型的方差。Boosting不仅能够减小偏差，还能减小方差。

### 7. 重新清洗数据
数据清洗从名字上也看的出就是把“脏”的“洗掉”，指发现并纠正数据文件中可识别的错误的最后一道程序，包括检查数据一致性，处理无效值和缺失值等。导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。

## 参考

- [过拟合（定义、出现的原因4种、解决方案7种）](https://blog.csdn.net/NIGHT_SILENT/article/details/80795640)