[toc]

# 什么是激活函数

 神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一层（隐层或输出层）。在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。 

为了增强网络的表示能力和学习能力，激活函数需要具备以下几点性质:

1. 连续并可导（允许少数点上不可导）的非线性函数。可导的激活函数可以直接利用数值优化的方法来学习网络参数。 
2. 激活函数及其导函数要尽可能的简单，有利于提高网络计算效率。 
3. 激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小， 否则会影响训练的效率和稳定性。

激活函数可以分为**两大类** ：

- **饱和**激活函数： sigmoid、 tanh
- **非饱和**激活函数: ReLU 、Leaky Relu  、ELU【指数线性单元】、PReLU【**参数化的**ReLU 】、RReLU【随机ReLU】

# 激活函数的用途

如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了，那么网络的逼近能力就相当有限。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）。

# 常见激活函数特点分析

## Sigmoid

数学表达式
$$
f(x)=\sigma(x) = \frac{1}{1+e^{-x}}
$$
其导数为：
$$
f'(x) = \sigma(x)(1-\sigma(x))
$$
![Sigmoid](.\activation\Sigmoid.png)

优点：

1. 输出结果在(0, 1)之间， 很好的表达神经元的激活与未激活的状态，适合二分类。
2. 单调连续，求导方便。

缺点：

1. 根据图像也可容易看出：极易饱和。当输入范围在(-∞, -4)或(4, +∞)时，就落入了饱和区，一阶导数接近0，这就使得容易产生梯度消失，进而导致训练出现问题。
2. 输出不是0均值的，因为这会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响：假设后层神经元的输入都为正，那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。

## Tanh

Tanh 函数可以看作是放大并平移的 Logistic 函数 ，数学表达式
$$
f(x)=\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\sigma(2x) - 1
$$
其导数为：
$$
f'(x) = 1- \tanh(x) \times \tanh(x)
$$
![Tanh](.\activation\Tanh.png)

优点：

1. 输出结果在(-1, 1)之间，输出范围有限，且以0为中心；
2. 单调连续，收敛速度比sigmoid更快；

缺点：

1. 根据图像也可容易看出：同sigmoid一样，极易饱和。当输入范围在(-∞, -2)或(2, +∞)时，就落入了饱和区，一阶导数接近0，这就使得容易产生梯度消失，进而导致训练出现问题。
2.  计算量大，函数要进行指数运算，这个对于计算机来说是比较慢的。 

## ReLU（Rectified linear unit，修正线性单元）

数学表达式
$$
f(x)=max(0, x)
$$
![ReLU](.\activation\Relu.png)

优点：

1. 收敛速度比sigmoid、tanh函数更快；
2. 当x>0时，保持梯度不衰减，从而可以有效缓解梯度消失的问题。

缺点：

1. 当x<0时，硬饱和，这会导致部分输入对应的权重无法更新，可能会出现“神经元死亡”。

## Leaky ReLU（PReLU）

数学表达式
$$
f(x)=max(\alpha x, x)
$$
![PReLU](.\activation\Leaky Relu.png)

优点：

1. PReLU也是针对ReLU的一个改进型，在负数区域内，PReLU有一个很小的斜率，这样也可以避免ReLU死掉的问题。 

## ELU（Exponential Linear Units）

数学表达式
$$
f(x)=\begin{cases}
x & x>0 \\
\alpha (e^x - 1) & x \leq 0 
\end{cases}
$$
![ELU](.\activation\Elu.png)

优点：

1.  ELU函数是针对ReLU函数的一个改进型，相比于ReLU函数，在输入为负数的情况下，是有一定的输出的，而且这部分输出还具有一定的抗干扰能力。
2.   输出的均值接近0。

缺点：

1.  计算量稍大。

## Softplus

Softplus函数可以看作是ReLU函数的平滑版本，数学表达式
$$
f(x)=\log (1 + e^x)
$$
其导数为：
$$
f'(x)  = \frac{1}{1+e^{-x}}
$$
![Softplus](.\activation\Softplus.png)

优点：

1. softplus函数对relu函数做了平滑处理。

## Softsign

数学表达式
$$
f(x)=\frac{x}{1+|x|}
$$
其导数为：
$$
f'(x) = \frac{1}{(1+|x|)^2}
$$
![Softsign](.\activation\Softsign.png)

## SoftMax

数学表达式
$$
\phi_i(z)=\frac{e^{z_j}}{\sum_{j} e^{z_j}}
$$
用于多分类神经网络输出。

## MaxOut

TODO

# 参考

- [常见激活函数特点分析]( https://www.jianshu.com/p/89956fbb7098 )
- [7.激励函数(激活函数)]( https://www.jianshu.com/p/e2e7cde335a0 )
- [常用激活函数（激励函数）理解与总结]( https://blog.csdn.net/tyhj_sf/article/details/79932893 )
- [深度学习的激活函数  ：sigmoid、tanh、ReLU 、Leaky Relu、RReLU、softsign 、softplus、GELU]( [https://blog.csdn.net/qq_29831163/article/details/89887655#%EF%BC%8810%EF%BC%89%E5%88%86%E6%AE%B5%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0%C2%A0](https://blog.csdn.net/qq_29831163/article/details/89887655#（10）分段线性函数 ) )