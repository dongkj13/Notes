## 批量梯度下降（BGD)

$$
g_t \leftarrow \nabla{f(x_{t-1})} = \dfrac{1}{n} \sum_{i=1}^n \nabla{f_i(x_{t-1})},\\
x_t \leftarrow x_{t-1} - \eta_t g_t
$$

- 每轮迭代中选择所有样本来计算梯度

## 随机梯度下降（SGD）

$$
x_t \leftarrow x_{t-1} - \eta_t \nabla{f_i(x_{t-1})}
$$

- 每次迭代中，随机均匀采样的一个样本索引来计算梯度

## 小批量随机梯度下降（MBGD）

$$
g_t \leftarrow \nabla{f_{\mathcal{B}}(x_{t-1})} = \dfrac{1}{\mathcal{B}} \sum_{i \in \mathcal{B}_t} \nabla{f_i(x_{t-1})},\\
x_t \leftarrow x_{t-1} - \eta_t g_t
$$

- 每轮迭代中随机均匀采样多个样本来组成一个小批量，然后使用这个小批量来计算梯度。

## 动量法

$$
v_t \leftarrow \gamma v_{t-1} + \eta_t g_t, \\
x_t \leftarrow x_{t-1} - v_t
$$

其中，动量超参数$\gamma$满足$0 \leq \gamma \leq 1$。当$\gamma=0$时，动量法等价于小批量随机梯度下降。

- 适用场景：当梯度值在不同维度上有较大差别时，需要选择足够小的学习率使得自变量在梯度值较大的维度上不发散，但这样会导致自变量在梯度值较小的维度上迭代过慢。
- 动量法使用了指数加权移动平均的思想。它将过去时间步的梯度做了加权平均，且权重按时间步指数衰减。
- 动量法使得相邻时间步的自变量更新在方向上更加一致。

## AdaGrad算法

$$
s_t \leftarrow s_{t-1} + g_t \odot g_t, \\
x_t \leftarrow x_{t-1} - \frac{\eta}{\sqrt{s_t + \epsilon}} \odot g_t
$$

其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。

- 根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。
- 使用AdaGrad算法时，自变量中每个元素的学习率在迭代过程中一直在降低（或不变），当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。

## RMSProp算法

$$
s_t \leftarrow \gamma s_{t-1} + (1-\gamma) g_t \odot g_t, \\
x_t \leftarrow x_{t-1} - \frac{\eta}{\sqrt{s_t + \epsilon}} \odot g_t
$$

- 为了解决AdaGrad算法在后期学习率过小的问题，RMSProp算法使用了小批量随机梯度按元素平方的指数加权移动平均来调整学习率。

## AdaDelta算法

$$
s_t \leftarrow \rho s_{t-1} + (1-\rho) g_t \odot g_t, \\
g_t' \leftarrow \frac{\sqrt{\Delta\boldsymbol{x}_{t-1} + \epsilon}}{\sqrt{s_t + \epsilon}} \odot g_t, \\
x_t \leftarrow x_{t-1} - g_t', \\
\Delta\boldsymbol{x}_t \leftarrow \rho \Delta\boldsymbol{x}_{t-1} + (1-\rho) g_t' \odot g_t'
$$

- 同样为了解决AdaGrad算法在后期学习率过小的问题，AdaDelta算法维护一个额外的状态变量$\Delta\boldsymbol{x}_t$来替代学习率

## Adam算法

$$
v_t \leftarrow \beta_1 v_{t-1} + (1-\beta_1) g_t, \\
s_t \leftarrow \beta_2 s_{t-1} + (1-\beta_2) g_t \odot g_t, \\
\hat{v}_t \leftarrow \frac{v_t}{1-\beta_1^t}, \hat{s}_t \leftarrow \frac{s_t}{1-\beta_2^t},  偏差修正\\
x_t \leftarrow x_{t-1} - \frac{\eta}{\sqrt{\hat{s}_t + \epsilon}} \hat{v}_t
$$

- Adam算法可以看做是RMSProp算法与动量法的结合。

## 参考

[优化算法](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter07_optimization/7.1_optimization-intro)

